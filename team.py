# åŸºæœ¬å¥—ä»¶
import asyncio
import os
import configparser

# AutoGen ç›¸é—œå¥—ä»¶
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.base import TaskResult
from autogen_agentchat.conditions import (
    ExternalTermination, 
    TextMentionTermination, 
    MaxMessageTermination
)
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console

# AutoGen æ“´å±•å¥—ä»¶
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Rich å¥—ä»¶ï¼ˆå¦‚æœéœ€è¦ç¾åŒ–è¼¸å‡ºï¼‰
from rich.console import Console
from rich.panel import Panel

def load_config():
    config = configparser.ConfigParser()
    
    # æª¢æŸ¥è¨­å®šæª”æ˜¯å¦å­˜åœ¨
    if not os.path.exists('config.ini'):
        raise FileNotFoundError('è«‹å»ºç«‹ config.ini æª”æ¡ˆï¼ˆå¯ä»¥å¾ config.example.ini è¤‡è£½ï¼‰')
    
    config.read('config.ini')
    return config

# è®€å–è¨­å®š
config = load_config()
API_KEY = config['API']['api_key_mistral']

# # ollamaæœ¬åœ°éƒ¨ç½²
def get_model_client_ollama() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="qwen2.5-coder-extra:latest",
        api_key="ollama",
        base_url="http://ollama.webtw.xyz:11434/v1",
        model_capabilities={
            "json_output": False,
            "vision": True,
            "function_calling": True,
        },
    )

## OpenRouter
# def get_model_client_OpenRouter() -> OpenAIChatCompletionClient:  # type: ignore
#     "Mimic OpenAI API using Local LLM Server."
#     return OpenAIChatCompletionClient(
#         model="microsoft/phi-4",
#         api_key=api_key,
#         base_url="https://openrouter.ai/api/v1",
#         model_capabilities={
#             "json_output": False,
#             "vision": False,
#             "function_calling": True,
#         },
#     )

# ## Nvidia NIM
# def get_model_client_NIM() -> OpenAIChatCompletionClient:  # type: ignore
#     return OpenAIChatCompletionClient(
#         model="meta/llama-3.3-70b-instruct",
#         api_key=api_key_nvidia_nim,
#         base_url="https://integrate.api.nvidia.com/v1",
#         model_capabilities={
#             "json_output": True,
#             "vision": False,
#             "function_calling": True,
#         },
#     )

## Mistral API
def get_model_client_Mistral() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="mistral-large-latest",
        api_key=API_KEY,
        base_url="https://api.mistral.ai/v1/chat/completions",
        model_capabilities={
            "json_output": True,
            "vision": False,
            "function_calling": True,
        },
    )    

## å…¶ä»–ç¬¬ä¸‰æ–¹æµ‹è¯•
# def get_model_client_other() -> OpenAIChatCompletionClient:  # type: ignore
#     return OpenAIChatCompletionClient(
#         model="meta-llama/Llama-3.3-70B-Instruct",
#         api_key=api_key_2,
#         base_url="https://api.hyperbolic.xyz/v1",
#         model_capabilities={
#             "json_output": False,
#             "vision": False,
#             "function_calling": True,
#         },
#     )

# # ollamaæœ¬åœ°éƒ¨ç½²
# def get_model_client_ollama() -> OpenAIChatCompletionClient:  # type: ignore
#     return OpenAIChatCompletionClient(
#         model="llama3.2:latest",
#         api_key="ollama",
#         base_url="http://localhost:11434/v1",
#         model_capabilities={
#             "json_output": False,
#             "vision": False,
#             "function_calling": True,
#         },
#     )

# åˆ›å»º OpenAI æ¨¡å‹å®¢æˆ·ç«¯
model_client = get_model_client_Mistral()

# åˆ›å»ºPythonå¼€å‘å·¥ç¨‹å¸ˆ
Programmer_Agent = AssistantAgent(
    "programmer",
    model_client=model_client,
    system_message="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¼€å‘å·¥ç¨‹å¸ˆã€‚
è¯·åŸºäºéœ€æ±‚ç¼–å†™æ¸…æ™°ã€å¯ç»´æŠ¤ä¸”ç¬¦åˆPEP8è§„èŒƒçš„Pythonä»£ç ã€‚
ä»£ç è¦åŒ…å«:
- æ¸…æ™°çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
- é€‚å½“çš„é”™è¯¯å¤„ç†
- ä»£ç æ€§èƒ½ä¼˜åŒ–
- å•å…ƒæµ‹è¯•
""",
)

# åˆ›å»ºä»£ç å®¡è®¡ä¸“å®¶
CodeReviewer_Agent = AssistantAgent(
    "code_reviewer",
    model_client=model_client,
    system_message="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚è¯·å¯¹ä»£ç è¿›è¡Œå…¨é¢çš„è¯„å®¡,åŒ…æ‹¬:
- ä»£ç è§„èŒƒæ€§å’Œå¯è¯»æ€§
- è®¾è®¡æ¨¡å¼çš„ä½¿ç”¨
- æ€§èƒ½å’Œæ•ˆç‡
- å®‰å…¨æ€§è€ƒè™‘
- æµ‹è¯•è¦†ç›–ç‡
- æ½œåœ¨é—®é¢˜
å½“ä»£ç ç¬¦åˆè¦æ±‚æ—¶,å›å¤'åŒæ„é€šè¿‡'ã€‚""",
)

# å®šä¹‰ç»ˆæ­¢æ¡ä»¶:å½“è¯„è®ºå‘˜åŒæ„æ—¶åœæ­¢ä»»åŠ¡
text_termination = TextMentionTermination("åŒæ„é€šè¿‡")

# åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸»è¦æ™ºèƒ½åŠ©æ‰‹å’Œè¯„è®ºå‘˜çš„å›¢é˜Ÿ
team = RoundRobinGroupChat([Programmer_Agent, CodeReviewer_Agent], termination_condition=text_termination)

# ç¤ºä¾‹ä»»åŠ¡:å®ç°ä¸€ä¸ªæ–‡ä»¶å¤„ç†ç±»
task = """
è¯·å®ç°ä¸€ä¸ªæ–‡ä»¶å¤„ç†ç±» FileProcessor,è¦æ±‚:
1. æ”¯æŒè¯»å–ã€å†™å…¥å’Œè¿½åŠ æ–‡æœ¬æ–‡ä»¶
2. åŒ…å«åŸºæœ¬çš„æ–‡ä»¶ç»Ÿè®¡åŠŸèƒ½(è¡Œæ•°ã€å­—ç¬¦æ•°ã€å•è¯æ•°)
3. æ”¯æŒæ–‡ä»¶åŠ å¯†/è§£å¯†åŠŸèƒ½
4. å®ç°å¼‚å¸¸å¤„ç†
5. ç¼–å†™å®Œæ•´çš„å•å…ƒæµ‹è¯•
"""

def print_formatted_result(task_result):
    print("\n" + "="*60)
    print("ä»£ç è¯„å®¡è¿‡ç¨‹".center(60))
    print("="*60 + "\n")

    for msg in task_result.messages:
        if msg.source == 'user':
            print("ğŸ“‹ éœ€æ±‚æè¿°ï¼š")
        elif msg.source == 'primary':
            print("ğŸ‘¨â€ğŸ’» å¼€å‘å·¥ç¨‹å¸ˆæäº¤ï¼š")
        elif msg.source == 'critic':
            print("ğŸ” ä»£ç å®¡æŸ¥åé¦ˆï¼š")

        print("-" * 40)
        print(f"{msg.content}\n")

        if msg.models_usage:
            print(f"Tokenç»Ÿè®¡ï¼š")
            print(f"Â· æç¤ºtokens: {msg.models_usage.prompt_tokens}")
            print(f"Â· ç”Ÿæˆtokens: {msg.models_usage.completion_tokens}")
            print(f"Â· æ€»è®¡tokens: {msg.models_usage.prompt_tokens + msg.models_usage.completion_tokens}\n")

    print("="*60)
    print("è¯„å®¡ç»“æœï¼š".center(60))
    print("="*60)
    print(f"\n{task_result.stop_reason}\n")

async def run_team_chat(task: str):
    model_client = get_model_client_Mistral()
    
    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    
    termination = MaxMessageTermination(11)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    
    result = await team.run(task=task)
    return result

async def run_code_review(task: str):
    # åˆ›å»º OpenAI æ¨¡å‹å®¢æˆ·ç«¯
    model_client = get_model_client_Mistral()

    # åˆ›å»ºå›¢é˜Ÿæˆå‘˜
    programmer = AssistantAgent(
        "programmer",
        model_client=model_client,
        system_message="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¼€å‘å·¥ç¨‹å¸ˆã€‚
    è¯·åŸºäºéœ€æ±‚ç¼–å†™æ¸…æ™°ã€å¯ç»´æŠ¤ä¸”ç¬¦åˆPEP8è§„èŒƒçš„Pythonä»£ç ã€‚
    ä»£ç è¦åŒ…å«:
    - æ¸…æ™°çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
    - é€‚å½“çš„é”™è¯¯å¤„ç†
    - ä»£ç æ€§èƒ½ä¼˜åŒ–
    - å•å…ƒæµ‹è¯•
    """,
    )

    reviewer = AssistantAgent(
        "code_reviewer",
        model_client=model_client,
        system_message="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚è¯·å¯¹ä»£ç è¿›è¡Œå…¨é¢çš„è¯„å®¡,åŒ…æ‹¬:
    - ä»£ç è§„èŒƒæ€§å’Œå¯è¯»æ€§
    - è®¾è®¡æ¨¡å¼çš„ä½¿ç”¨
    - æ€§èƒ½å’Œæ•ˆç‡
    - å®‰å…¨æ€§è€ƒè™‘
    - æµ‹è¯•è¦†ç›–ç‡
    - æ½œåœ¨é—®é¢˜
    å½“ä»£ç ç¬¦åˆè¦æ±‚æ—¶,å›å¤'åŒæ„é€šè¿‡'ã€‚""",
    )

    # å®šä¹‰ç»ˆæ­¢æ¡ä»¶
    text_termination = TextMentionTermination("åŒæ„é€šè¿‡")

    # åˆ›å»ºå›¢é˜Ÿ
    team = RoundRobinGroupChat([programmer, reviewer], termination_condition=text_termination)

    # è¿è¡Œä»»åŠ¡
    result = await team.run(task=task)
    return result

def main():
    # ç¤ºä¾‹ä»»åŠ¡
    task = """
    è¯·å®ç°ä¸€ä¸ªæ–‡ä»¶å¤„ç†ç±» FileProcessor,è¦æ±‚:
    1. æ”¯æŒè¯»å–ã€å†™å…¥å’Œè¿½åŠ æ–‡æœ¬æ–‡ä»¶
    2. åŒ…å«åŸºæœ¬çš„æ–‡ä»¶ç»Ÿè®¡åŠŸèƒ½(è¡Œæ•°ã€å­—ç¬¦æ•°ã€å•è¯æ•°)
    3. æ”¯æŒæ–‡ä»¶åŠ å¯†/è§£å¯†åŠŸèƒ½
    4. å®ç°å¼‚å¸¸å¤„ç†
    5. ç¼–å†™å®Œæ•´çš„å•å…ƒæµ‹è¯•
    """
    
    # è¿è¡Œä»£ç è¯„å®¡
    result = asyncio.run(run_code_review(task))
    
    # æ‰“å°ç»“æœ
    print_formatted_result(result)

if __name__ == "__main__":
    main()
